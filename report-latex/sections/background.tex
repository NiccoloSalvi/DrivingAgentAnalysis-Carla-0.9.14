\section{Background}
\label{sec:background}

This section provides the necessary background for our analysis. It covers the primary strategies in autonomous driving development, explains the importance of virtual simulation and standardized benchmarking for research, and introduces the specific models examined in this study.

\subsection{Autonomous Driving Agents}
In recent years, autonomous driving has experienced remarkable advancements thanks to companies such as Tesla and Waymo, which adopt distinct strategies to achieve fully autonomous vehicles. Tesla utilizes a camera-based approach known as \textit{Tesla Vision}~\footnote{See Tesla Support, "Replacing Ultrasonic Sensors with Tesla Vision" Dec 2024.}, relying solely on RGB cameras and artificial intelligence without incorporating LiDAR or radar sensors. This vision-based approach aims to emulate human perception, employing neural networks to interpret images and make driving decisions. Tesla's long-term goal is achieving SAE Level 5 autonomy, which implies complete autonomous driving capability under any conditions without human intervention.

Conversely, Waymo\footnote{See Waymo, "Self-Driving Car Technology for a Reliable Ride - Waymo Driver" 2024; Automotive Dive, "A look inside the sixth-generation Waymo Driver" 2024.} adopts a comprehensive sensor suite including RGB cameras, radar, and LiDAR sensors. This multi-sensor integration provides robust three-dimensional perception, enhancing the safety and reliability of the autonomous system. Waymo has already deployed autonomous ride-hailing services in selected U.S. cities such as Phoenix and San Francisco, continuously expanding its autonomous fleet. Both companies share the ultimate vision of transforming mobility by reducing traffic accidents and increasing urban transportation efficiency.

\subsection{CARLA Simulator}
CARLA (Car Learning to Act)~\cite{dosovitskiy2017carlaopenurbandriving} is an open-source simulator explicitly developed to support autonomous driving research, development, training, and validation. It provides a realistic virtual urban environment populated with vehicles, pedestrians, traffic signals, and varying weather conditions. Employing virtual simulations like CARLA has significant advantages over real-world testing. Virtual environments ensure safety by allowing testing of dangerous scenarios without risks to individuals or property. Additionally, simulations offer greater efficiency, enabling rapid execution of numerous tests at reduced costs. The controlled environment provided by CARLA allows precise replication of specific conditions, facilitating systematic comparisons and accurate performance evaluations of various autonomous driving algorithms. Consequently, CARLA has emerged as an essential tool within autonomous driving research.

\subsection{CARLA Autonomous Driving Leaderboard}
The CARLA Autonomous Driving Leaderboard~\footnote{CARLA Autonomous Driving Leaderboard. Available at: \url{http://leaderboard.carla.org}} is an evaluation platform specifically designed to compare the performance of diverse autonomous driving agents within the CARLA simulator. Through standardized scenarios, agents are assessed on multiple aspects such as adherence to traffic rules, obstacle management, and navigation capability. Annually, a global competition organized by the CARLA community invites participation from universities, research institutions, and industry players to foster innovation and establish benchmarks in the field.

Models such as TransFuser~\cite{chitta2022transfuserimitationtransformerbasedsensor} and InterFuser~\cite{shao2022safetyenhancedautonomousdrivingusing} have actively participated in these competitions, notably in the 2021 and 2022 editions, respectively, achieving significant results and influencing advancements in the domain. The CARLA Leaderboard thus serves as a crucial standard for evaluating and benchmarking autonomous driving agents, accelerating progress and facilitating collaborative development within the research community.

\subsubsection{Evaluation Metrics}
The CARLA Leaderboard employs a comprehensive set of metrics to assess the performance of autonomous driving agents. The primary metric is the Driving Score, denoted as \( D_i \), which evaluates an agent's ability to complete routes while adhering to traffic rules and avoiding infractions. It is calculated as the product of the Route Completion \( R_i \) and the Infraction Penalty \( P_i \):

\[ D_i = R_i \times P_i \]

Here, \( R_i \) represents the percentage of the route completed by the agent, with a maximum value of 100\%, and \( P_i \) is a coefficient reflecting the penalties incurred due to infractions, ranging from 0 to 1.

The Infraction Penalty \( P_i \) is determined by aggregating the penalties from various infractions committed during the route. Each infraction type has an associated penalty coefficient \( p_j \), and the overall penalty is computed as:

\[ P_i = \prod_j p_j^{n_j} \]

where \( n_j \) is the number of times infraction \( j \) occurred. This multiplicative approach ensures that repeated infractions have a compounding negative effect on the driving score.

Infractions are categorized based on their severity, with corresponding penalty coefficients. For instance, collisions with pedestrians have a penalty coefficient of 0.50, collisions with vehicles 0.60, collisions with static objects 0.65, running a red light 0.70, failing to yield to emergency vehicles 0.70, and running a stop sign 0.80. Additional penalties are applied for scenario timeouts (0.70), failure to maintain minimum speed (up to 0.70), and off-road driving, which reduces the route completion percentage accordingly.

Furthermore, certain events can lead to the termination of a simulation run. These include deviating more than 30 meters from the assigned route, the agent being inactive for 180 simulation seconds, simulation timeouts due to communication issues, exceeding the maximum allowed time for a route, and errors within the agent's code. In such cases, the route is marked as failed, and the simulation proceeds to the next route.

\subsection{Overview of Studied Models}
The models analyzed in this study, InterFuser and TransFuser, have each represented a significant milestone in the evolution of autonomous driving agents, particularly within the CARLA simulation environment.

\subsubsection{InterFuser}
InterFuser~\cite{shao2022safetyenhancedautonomousdrivingusing} is a neural network-based model leveraging deep learning, designed to integrate interpretable intermediate representations with direct end-to-end control. It combines inputs from RGB cameras and LiDAR sensors to produce two primary outputs: a sequence of waypoints representing the desired future trajectory of the vehicle, and an object density map encoding spatial and dynamic information about surrounding agents such as vehicles, pedestrians, and cyclists. Additionally, InterFuser predicts traffic rules such as traffic light states and stop signs. This integration enables the implementation of a safety controller that considers predicted future trajectories of surrounding objects to plan safe maneuvers.

In addition to generating a sequence of waypoints, InterFuser outputs an object density map ($M$), which represents the space around the vehicle as a grid of $(1,\text{m}) \times (1,\text{m})$ cells. Each cell encodes seven features that describe the presence and potential state of nearby objects such as vehicles, pedestrians, and cyclists: existence probability, offset from the cell center, bounding box size, heading, and object velocity. To detect the presence of an object in a cell of the map, two probability thresholds are applied. However, to assess the safety of the predicted trajectory, the controller employs a tracking system that records the motion history of objects surrounding the vehicle. It then estimates their future positions by applying a moving average over the observed dynamics.

\subsubsection{TransFuser}
TransFuser~\cite{chitta2022transfuserimitationtransformerbasedsensor} relies on a transformer-based architecture for multi-sensor fusion. It processes RGB images and LiDAR data using dedicated encoders and then fuses the resulting feature maps through cross-attention mechanisms. The model outputs future waypoints in the Birdâ€™s Eye View plane, which are used by a PID controller to derive steering and acceleration commands. In addition, TransFuser performs auxiliary tasks such as semantic segmentation and object detection, improving the interpretability and generalization of the learned policies.

The original TransFuser paper introduces several architectural variants that explore different approaches to multi-modal sensor fusion. These variants differ primarily in how they combine RGB and LiDAR information, ranging from attention-based mechanisms to geometric projections and simplified fusion strategies. We describe these variants below to provide a comprehensive overview of the TransFuser family.

\paragraph{LatentTF} LatentTF was explicitly introduced in the original TransFuser paper to study the effect of removing LiDAR data. It shares the same Transformer backbone and prediction objectives but uses static positional encoding in place of real-world geometric input from LiDAR, offering a clean ablation for sensor dependence. that removes the explicit LiDAR input and instead relies on a fixed latent positional encoding to guide the fusion process. This architectural modification reduces computational complexity, making the model more lightweight, but at the expense of robustness in geometrically challenging scenes or under degraded visibility. Despite this, LatentTF retains the core Transformer-based structure and learning objectives of TransFuser, making it suitable for benchmarking the impact of sensor reduction on driving performance.

\paragraph{Late Fusion} Late Fusion is a variation of TransFuser in which RGB and LiDAR features are extracted independently using standard encoders, and then combined through simple element-wise summation rather than via attention mechanisms. While computationally efficient, this approach lacks the rich inter-modal interactions provided by transformers, limiting its expressiveness in complex driving scenarios.

\paragraph{Geometric Fusion} Geometric Fusion replaces the attention-based fusion of TransFuser with projection-based feature integration. It unprojects LiDAR BEV cells into 3D space, samples LiDAR points, and reprojects them into the image plane to aggregate visual features. These are fused through a multi-layer perceptron and combined with LiDAR features at multiple scales. This approach emphasizes geometrically grounded alignment but increases implementation complexity.

\paragraph{Model Selection}
For this study, we selected three models for comparative analysis: InterFuser, TransFuser (main model), and LatentTF. InterFuser and TransFuser were chosen primarily due to their widespread adoption and established performance benchmarks in the autonomous driving literature, making them representative baselines for evaluating multi-modal fusion approaches. 
In this project, LatentTF was included in the analysis to investigate how the absence of direct LiDAR data would impact the modelâ€™s behavior, particularly in challenging scenarios such as high-traffic environments and creeping situations. The authors of TransFuser describe a \textit{creeping mechanism}, triggered when the agent remains stationary for an extended period, which is designed to push the vehicle slightly forward to prevent deadlocks. During evaluations, the hypothesis was that issues in creeping behavior could be related to LiDAR data processing, and thus, there was an interest in assessing whether LatentTF, being solely vision-based, would perform differently under the same conditions. Geometric Fusion and Late Fusion models were excluded from this analysis. While important for understanding early fusion techniques, they are less relevant to our core research question, which focuses on comparing the advanced attention-based fusion of TransFuser with the intermediate representation approach of InterFuser, and analyzing the specific impact of LiDAR data via LatentTF.