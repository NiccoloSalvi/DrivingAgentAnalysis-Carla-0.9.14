\section{Introduction}
\label{sec:intro}

Autonomous driving is one of the most ambitious challenges in artificial intelligence, with the potential to revolutionize transportation. A key aspect of this field is the development and evaluation of driving agents capable of making real-time decisions based on multi-modal sensor inputs, such as cameras, LiDAR, and route planning data. Accurately benchmarking these agents is essential for pushing the field forward and ensuring their safe, reliable operation in complex urban environments. Simulation platforms like CARLA (Car Learning to Act) offer a standardized and controllable environment to evaluate such systems.
    
Within this space, TransFuser and InterFuser are two leading AI driving agents designed to tackle the sensor fusion problem. Both models have achieved strong performance on the CARLA Leaderboard, which ranks agents based on their behavior in realistic driving scenarios. However, while leaderboard metrics such as route completion and infraction scores provide useful performance indicators, they don’t always capture the full picture—especially when agents behave in strange or unexpected ways. This work seeks not only to compare these two agents in terms of traditional metrics such as route completion and infractions but also to investigate the underlying causes of unusual behaviors observed during their operation.

This study evaluates existing pre-trained end-to-end state-of-the-art agents such as TransFuser and InterFuser within the CARLA simulation environment using the CARLA Leaderboard framework. The agents are used as-is, without any retraining or modification. Our focus is on comparing their performance under identical conditions and conducting a qualitative investigation into their behavior. Special attention is given to identifying instances where the agents exhibit unusual or flawed decision-making and attempting to determine the underlying causes by examining the simulation context, sensor inputs, and environmental factors.